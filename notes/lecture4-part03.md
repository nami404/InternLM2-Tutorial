# å¤ç°å¤šæ¨¡æ€å¾®è°ƒ
[å®˜æ–¹æ•™ç¨‹åœ°å€](https://github.com/InternLM/Tutorial/blob/camp2/xtuner/llava/xtuner_llava.md)

## XTunerå¤šæ¨¡æ€è®­ç»ƒä¸æµ‹è¯•
### 1.1. ç»™LLMè£…ä¸Šç”µå­çœ¼ï¼šå¤šæ¨¡æ€LLMåŸç†ç®€ä»‹
#### 1.1.1. æ–‡æœ¬å•æ¨¡æ€
![](../images/lecture4/1716890018491.png)

#### 1.1.2. æ–‡æœ¬+å›¾åƒå¤šæ¨¡æ€
![](../images/lecture4/1716890071060.png)

### 1.2. ä»€ä¹ˆå‹å·çš„ç”µå­çœ¼ï¼šLLaVAæ–¹æ¡ˆç®€ä»‹
ç”¨GPT-4Vå¯¹å›¾åƒæ•°æ®ç”Ÿæˆæè¿°ï¼Œä»¥æ­¤æ„å»ºå‡ºå¤§é‡<question text><image> -- <answer text>çš„æ•°æ®å¯¹ã€‚åˆ©ç”¨è¿™äº›æ•°æ®å¯¹ï¼Œé…åˆæ–‡æœ¬å•æ¨¡æ€LLMï¼Œè®­ç»ƒå‡ºä¸€ä¸ªImage Projectorã€‚

æ‰€ä½¿ç”¨çš„æ–‡æœ¬å•æ¨¡å‹LLMå’Œè®­ç»ƒå‡ºæ¥çš„Image Projectorï¼Œç»Ÿç§°ä¸ºLLaVAæ¨¡å‹ã€‚
#### 1.2.1. LLaVAè®­ç»ƒé˜¶æ®µç¤ºæ„å›¾
![](../images/lecture4/1716890103747.png)

1.2.2. LLaVAæµ‹è¯•é˜¶æ®µç¤ºæ„å›¾
![](../images/lecture4/1716890137571.png)

> Image Projectorçš„è®­ç»ƒå’Œæµ‹è¯•ï¼Œæœ‰ç‚¹ç±»ä¼¼ä¹‹å‰æˆ‘ä»¬è®²è¿‡çš„LoRAå¾®è°ƒæ–¹æ¡ˆã€‚

äºŒè€…éƒ½æ˜¯åœ¨å·²æœ‰LLMçš„åŸºç¡€ä¸Šï¼Œç”¨æ–°çš„æ•°æ®è®­ç»ƒä¸€ä¸ªæ–°çš„å°æ–‡ä»¶ã€‚

åªä¸è¿‡ï¼ŒLLMå¥—ä¸ŠLoRAä¹‹åï¼Œæœ‰äº†æ–°çš„çµé­‚ï¼ˆè§’è‰²ï¼‰ï¼›è€ŒLLMå¥—ä¸ŠImage Projectorä¹‹åï¼Œæ‰æœ‰äº†çœ¼ç›ã€‚

### 1.3. å¿«é€Ÿä¸Šæ‰‹
### 1.3.1. ç¯å¢ƒå‡†å¤‡
#### 1.3.1.2. XTunerå®‰è£…
```bash
# å¦‚æœä½ æ˜¯åœ¨ InternStudio å¹³å°ï¼Œåˆ™ä»æœ¬åœ° clone ä¸€ä¸ªå·²æœ‰ pytorch çš„ç¯å¢ƒï¼š
# pytorch    2.0.1   py3.10_cuda11.7_cudnn8.5.0_0

cd ~ && studio-conda xtuner0.1.17
# å¦‚æœä½ æ˜¯åœ¨å…¶ä»–å¹³å°ï¼š
# conda create --name xtuner0.1.17 python=3.10 -y

# æ¿€æ´»ç¯å¢ƒ
conda activate xtuner0.1.17
# è¿›å…¥å®¶ç›®å½• ï¼ˆ~çš„æ„æ€æ˜¯ â€œå½“å‰ç”¨æˆ·çš„homeè·¯å¾„â€ï¼‰
cd ~
# åˆ›å»ºç‰ˆæœ¬æ–‡ä»¶å¤¹å¹¶è¿›å…¥ï¼Œä»¥è·Ÿéšæœ¬æ•™ç¨‹
mkdir -p /root/xtuner0117 && cd /root/xtuner0117

# æ‹‰å– 0.1.17 çš„ç‰ˆæœ¬æºç 
git clone -b v0.1.17  https://github.com/InternLM/xtuner
# æ— æ³•è®¿é—®githubçš„ç”¨æˆ·è¯·ä» gitee æ‹‰å–:
# git clone -b v0.1.15 https://gitee.com/Internlm/xtuner

# è¿›å…¥æºç ç›®å½•
cd /root/xtuner0117/xtuner

# ä»æºç å®‰è£… XTuner
pip install -e '.[all]' && cd ~
```
> å‡å¦‚é€Ÿåº¦å¤ªæ…¢å¯ä»¥ `Ctrl + C` é€€å‡ºåæ¢æˆ `pip install -e '.[all]' -i https://mirrors.aliyun.com/pypi/simple/`

å‡å¦‚åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­æ²¡æœ‰å‡ºç°ä»»ä½•çš„æŠ¥é”™çš„è¯ï¼Œé‚£ä¹Ÿå°±æ„å‘³ç€æˆ‘ä»¬æˆåŠŸå®‰è£…å¥½æ”¯æŒ XTuner æ‰€è¿è¡Œçš„ç¯å¢ƒå•¦ã€‚å…¶å®å¯¹äºå¾ˆå¤šçš„åˆå­¦è€…è€Œè¨€ï¼Œå®‰è£…å¥½ç¯å¢ƒæ„å‘³ç€æˆåŠŸäº†ä¸€å¤§åŠï¼

### 1.3.2. æ¦‚è¿°

> åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°† **è‡ªå·±æ„é€  `<question text><image>--<answer text>` æ•°æ®å¯¹ï¼ŒåŸºäºInternLM2_Chat_1.8Bè¿™ä¸ªæ–‡æœ¬å•æ¨¡æ€æ¨¡å‹ï¼Œä½¿ç”¨LLaVAæ–¹æ¡ˆï¼Œè®­ç»ƒä¸€ä¸ªç»™InternLM2_Chat_1.8Bä½¿ç”¨çš„Image Projectoræ–‡ä»¶ã€‚**

LLaVAæ–¹æ¡ˆä¸­ï¼Œç»™LLMå¢åŠ è§†è§‰èƒ½åŠ›çš„è¿‡ç¨‹ï¼Œå³æ˜¯è®­ç»ƒImage Projectoræ–‡ä»¶çš„è¿‡ç¨‹ã€‚
è¯¥è¿‡ç¨‹åˆ†ä¸º2ä¸ªé˜¶æ®µï¼šPretrainå’ŒFinetuneã€‚
![](../images/lecture4/1716890233872.png)

### 1.3.3. Pretrainé˜¶æ®µ
åœ¨Pretrainé˜¶æ®µï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨å¤§é‡çš„`å›¾ç‰‡+ç®€å•æ–‡æœ¬ï¼ˆcaption, å³å›¾ç‰‡æ ‡é¢˜ï¼‰`æ•°æ®å¯¹ï¼Œä½¿LLMç†è§£å›¾åƒä¸­çš„**æ™®éç‰¹å¾**ã€‚å³ï¼Œå¯¹å¤§é‡çš„å›¾ç‰‡è¿›è¡Œ**ç²—çœ‹**ã€‚

Pretrainé˜¶æ®µè®­ç»ƒå®Œæˆåï¼Œæ­¤æ—¶çš„æ¨¡å‹å·²ç»æœ‰è§†è§‰èƒ½åŠ›äº†ï¼ä½†æ˜¯ç”±äºè®­ç»ƒæ•°æ®ä¸­éƒ½æ˜¯å›¾ç‰‡+å›¾ç‰‡æ ‡é¢˜ï¼Œæ‰€ä»¥æ­¤æ—¶çš„æ¨¡å‹è™½ç„¶æœ‰è§†è§‰èƒ½åŠ›ï¼Œä½†æ— è®ºç”¨æˆ·é—®å®ƒä»€ä¹ˆï¼Œå®ƒéƒ½åªä¼šå›ç­”è¾“å…¥å›¾ç‰‡çš„æ ‡é¢˜ã€‚å³ï¼Œ**æ­¤æ—¶çš„æ¨¡å‹åªä¼šç»™è¾“å…¥å›¾åƒâ€œå†™æ ‡é¢˜â€**ã€‚

> Pretrainé˜¶æ®µç›¸å½“äºæ˜¯å¼€å‘LLMæ—¶é¢„è®­ç»ƒå·¥ä½œï¼Œå¯¹ç¡¬ä»¶è¦æ±‚éå¸¸é«˜ï¼Œæœ‰8å¡çš„å­¦æœ‰ä½™åŠ›åŒå­¦å¯ä»¥è‡ªè¡Œå°è¯•ã€‚è¯¦è§[XTuner-LLaVA](https://github.com/InternLM/xtuner/blob/main/docs/zh_cn/user_guides/dataset_prepare.md#llava-dataset)å’Œ[LLaVA](https://llava-vl.github.io/)ã€‚
> <details>
>
> ```bash
> NPROC_PER_NODE=8 xtuner train llava_internlm2_chat_1_8b_clip_vit_large_p14_336_e1_gpu8_pretrain --deepspeed deepspeed_zero2
> 
> NPROC_PER_NODE=8 xtuner train llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune --deepspeed deepspeed_zero2
> ```

> </details>

åœ¨æœ¬æ¬¡å®æˆ˜è¥ä¸­ï¼Œæˆ‘ä»¬å·²ç»ä¸ºå¤§å®¶æä¾›äº†Pretrainé˜¶æ®µçš„äº§ç‰©â€”â€”`iter_2181.pth`æ–‡ä»¶ã€‚å®ƒå°±æ˜¯å¹¼ç¨šå›­é˜¶æ®µçš„Image Projectorï¼å¤§å®¶å¸¦ç€`iter_2181.pth`æ–‡ä»¶ç»§ç»­è¿›å…¥ä¸‹ä¸€é˜¶æ®µè¿›è¡ŒFinetuneå³å¯ã€‚

### 1.3.4. Finetuneé˜¶æ®µ
åœ¨Finetuneé˜¶æ®µï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨`å›¾ç‰‡+å¤æ‚æ–‡æœ¬`æ•°æ®å¯¹ï¼Œæ¥å¯¹Pretrainå¾—åˆ°çš„Image Projectorå³iter_2181.pthè¿›è¡Œè¿›ä¸€æ­¥çš„è®­ç»ƒã€‚

#### 1.3.4.1. è®­ç»ƒæ•°æ®æ„å»º

##### 1.3.4.1.1. æ ¼å¼
```json
[
    {
        "id": "éšä¾¿ä»€ä¹ˆå­—ç¬¦ä¸²",
        "image": "å›¾ç‰‡æ–‡ä»¶çš„ç›¸å¯¹ä½ç½®ã€‚ç›¸å¯¹è°ï¼Ÿç›¸å¯¹ä½ åé¢configæ–‡ä»¶é‡ŒæŒ‡å®šçš„image_folderå‚æ•°çš„è·¯å¾„ã€‚",
        "conversation": [
            {
                "from": "human",
                "value": "<image>\nç¬¬1ä¸ªé—®é¢˜ã€‚"
            },
            {
                "from": "gpt",
                "value": "ç¬¬1ä¸ªå›ç­”"
            },
            {
                "from": "human",
                "value": "ç¬¬2ä¸ªé—®é¢˜ã€‚"
            },
            {
                "from": "gpt",
                "value": "ç¬¬2ä¸ªå›ç­”"
            },
            # ......
            {
                "from": "human",
                "value": "ç¬¬nä¸ªé—®é¢˜ã€‚"
            },
            {
                "from": "gpt",
                "value": "ç¬¬nä¸ªå›ç­”"
            },
        ]
    },

    # ä¸‹é¢æ˜¯ç¬¬2ç»„è®­ç»ƒæ•°æ®äº†ã€‚

    {
        "id": "éšä¾¿ä»€ä¹ˆå­—ç¬¦ä¸²",
        "image": "å›¾ç‰‡æ–‡ä»¶çš„ç›¸å¯¹ä½ç½®ã€‚ç›¸å¯¹è°ï¼Ÿç›¸å¯¹ä½ åé¢configæ–‡ä»¶é‡ŒæŒ‡å®šçš„image_folderå‚æ•°çš„è·¯å¾„ã€‚",
        "conversation": [
            {
                "from": "human",
                "value": "<image>\nç¬¬1ä¸ªé—®é¢˜ã€‚"
            },
            # ......
            {
                "from": "gpt",
                "value": "ç¬¬nä¸ªå›ç­”"
            }
        ]
    }
]
```
> æ³¨æ„ï¼šæ¯ç»„è®­ç»ƒæ•°æ®çš„ç¬¬1ä¸ªæ¥è‡ªhumançš„é—®é¢˜å‰ï¼Œè¦åŠ ä¸Šå›¾ç‰‡å ä½ç¬¦ï¼Œå³`<image>`

<details>
<summary>ç¤ºä¾‹</summary>

```json
[
  {
    "id": "000000033471",
    "image": "coco/train2017/000000033471.jpg",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nWhat are the colors of the bus in the image?"
      },
      {
        "from": "gpt",
        "value": "The bus in the image is white and red."
      },
      {
        "from": "human",
        "value": "What feature can be seen on the back of the bus?"
      },
      {
        "from": "gpt",
        "value": "The back of the bus features an advertisement."
      },
      {
        "from": "human",
        "value": "Is the bus driving down the street or pulled off to the side?"
      },
      {
        "from": "gpt",
        "value": "The bus is driving down the street, which is crowded with people and other vehicles."
      }
    ]
  },
  {
    "id": "000000052846",
    "image": "coco/train2017/000000052846.jpg",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nWhere is the cat positioned in the image?"
      },
      {
        "from": "gpt",
        "value": "The cat is positioned on top of the back of the couch in the living room."
      }
    ]
  }
 ]
```
</details>

##### 1.3.4.1.2. åˆ¶ä½œ
æˆ‘ä»¬å¯ä»¥æ•ˆæ³•LLaVAä½œè€…çš„åšæ³•ï¼Œå°†è‡ªå·±çš„å›¾ç‰‡å‘é€ç»™GPTï¼Œè¦æ±‚å…¶æŒ‰ç…§ä¸Šè¿°æ ¼å¼ç”Ÿæˆè‹¥å¹²æ¡é—®ç­”å¯¹ã€‚
<details>
<summary>prompts</summary>

![](../images/lecture4/1716890285266.png)

Create a dataset for me, following this format.
```json
[
  {
    "id": "<random_number_string>",
    "image": "test_img/oph.jpg",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nDescribe this image."
      },
      {
        "from": "gpt",
        "value": "<answer1>"
      },
      {
        "from": "human",
        "value": "<question2>"
      },
      {
        "from": "gpt",
        "value": "<answer2>"
      },
      {
        "from": "human",
        "value": "<question3>"
      },
      {
        "from": "gpt",
        "value": "<answer3>"
      }
    ]
  }
]
```
The questions and answers, please generate for me, based on the image I sent to you. Thes questions should be from the shallow to the deep, and the answers should be as detailed and correct as possible. The questions and answers should be stick to the contents in the image itself, like objects, peoples, equipment, environment, purpose, color, attitude, etc. 5 question and answer pairs.
</details>
<br>


ä¸ºäº†æ–¹ä¾¿å¤§å®¶è·Ÿéšè¯¾ç¨‹ï¼Œé’ˆå¯¹è¿™å¼ ç¤ºä¾‹å›¾ç‰‡çš„é—®ç­”å¯¹æ•°æ®ï¼ˆrepeat_data.jsonï¼‰ï¼Œå¤§å®¶æŒ‰ç…§ä¸‹é¢çš„è„šæœ¬è¿è¡Œå°±å¯ä»¥ç”Ÿæˆå•¦~ï¼ˆé‡å¤200æ¬¡ï¼‰

```bash
cd ~ && git clone https://github.com/InternLM/tutorial -b camp2 && conda activate xtuner0.1.17 && cd tutorial

python /root/tutorial/xtuner/llava/llava_data/repeat.py \
  -i /root/tutorial/xtuner/llava/llava_data/unique_data.json \
  -o /root/tutorial/xtuner/llava/llava_data/repeated_data.json \
  -n 200
```

#### 1.3.4.2. å‡†å¤‡é…ç½®æ–‡ä»¶

> å¦‚æœä½ æ‡’åˆ°ä¸æƒ³è‡ªå·±æ”¹é…ç½®æ–‡ä»¶ï¼Œæˆ–è€…æ€ä¹ˆæ”¹éƒ½å¤±è´¥ã€‚æˆ‘ä»¬å‡†å¤‡äº†ä¸€ä¸ªfool_configæ–‡ä»¶åœ¨ä»“åº“é‡Œã€‚è¿è¡Œï¼š
```python
cp /root/tutorial/xtuner/llava/llava_data/internlm2_chat_1_8b_llava_tutorial_fool_config.py /root/tutorial/xtuner/llava/llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune_copy.py
```

##### 1.3.4.2.1. åˆ›å»ºé…ç½®æ–‡ä»¶

```bash
# æŸ¥è¯¢xtunerå†…ç½®é…ç½®æ–‡ä»¶
xtuner list-cfg -p llava_internlm2_chat_1_8b

# æ‹·è´é…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•
xtuner copy-cfg \
  llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune \
  /root/tutorial/xtuner/llava
```

å½“å‰ä½ çš„`/root/tutorial/xtuner/llava/`ç›®å½•ä¸‹çš„æ–‡ä»¶ç»“æ„åº”è¯¥æ˜¯è¿™æ ·ï¼š

```bash
|-- llava_data
|   |-- repeat.py
|   |-- repeated_data.json
|   |-- test_img
|   |   `-- oph.jpg
|   `-- unique_data.json
`-- llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune_copy.py
```

##### 1.3.4.2.2. ä¿®æ”¹é…ç½®æ–‡ä»¶

ä¿®æ”¹`llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune_copy.py`æ–‡ä»¶ä¸­çš„ï¼š
- pretrained_pth
- llm_name_or_path
- visual_encoder_name_or_path
- data_root
- data_path
- image_folder

```diff
# Model
- llm_name_or_path = 'internlm/internlm2-chat-1_8b'
+ llm_name_or_path = '/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b'
- visual_encoder_name_or_path = 'openai/clip-vit-large-patch14-336'
+ visual_encoder_name_or_path = '/root/share/new_models/openai/clip-vit-large-patch14-336'

# Specify the pretrained pth
- pretrained_pth = './work_dirs/llava_internlm2_chat_1_8b_clip_vit_large_p14_336_e1_gpu8_pretrain/iter_2181.pth'  # noqa: E501
+ pretrained_pth = '/root/share/new_models/xtuner/iter_2181.pth'

# Data
- data_root = './data/llava_data/'
+ data_root = '/root/tutorial/xtuner/llava/llava_data/'
- data_path = data_root + 'LLaVA-Instruct-150K/llava_v1_5_mix665k.json'
+ data_path = data_root + 'repeated_data.json'
- image_folder = data_root + 'llava_images'
+ image_folder = data_root

# Scheduler & Optimizer
- batch_size = 16  # per_device
+ batch_size = 1  # per_device


# evaluation_inputs
- evaluation_inputs = ['è¯·æè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡','Please describe this picture']
+ evaluation_inputs = ['Please describe this picture','What is the equipment in the image?']

```

#### 1.3.4.3. å¼€å§‹Finetune

```bash
cd /root/tutorial/xtuner/llava/
xtuner train /root/tutorial/xtuner/llava/llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune_copy.py --deepspeed deepspeed_zero2
```

### 1.3.5. å¯¹æ¯”Finetuneå‰åçš„æ€§èƒ½å·®å¼‚

#### 1.3.5.1. Finetuneå‰
> å³ï¼š**åŠ è½½ 1.8B å’Œ Pretrainé˜¶æ®µäº§ç‰©(iter_2181) åˆ°æ˜¾å­˜ã€‚**

```bash
# è§£å†³å°bug
export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER=GNU

# pthè½¬huggingface
xtuner convert pth_to_hf \
  llava_internlm2_chat_1_8b_clip_vit_large_p14_336_e1_gpu8_pretrain \
  /root/share/new_models/xtuner/iter_2181.pth \
  /root/tutorial/xtuner/llava/llava_data/iter_2181_hf

# å¯åŠ¨ï¼
xtuner chat /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b \
  --visual-encoder /root/share/new_models/openai/clip-vit-large-patch14-336 \
  --llava /root/tutorial/xtuner/llava/llava_data/iter_2181_hf \
  --prompt-template internlm2_chat \
  --image /root/tutorial/xtuner/llava/llava_data/test_img/oph.jpg
```
> Q1: Describe this image.     
> Q2: What is the equipment in the image?

#### 1.3.5.2. Finetuneå
> å³ï¼š**åŠ è½½ 1.8B å’Œ Fintuneé˜¶æ®µäº§ç‰© åˆ°æ˜¾å­˜ã€‚**

```bash
# è§£å†³å°bug
export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER=GNU

# pthè½¬huggingface
xtuner convert pth_to_hf \
  /root/tutorial/xtuner/llava/llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune_copy.py \
  /root/tutorial/xtuner/llava/work_dirs/llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune_copy/iter_1200.pth \
  /root/tutorial/xtuner/llava/llava_data/iter_1200_hf

# å¯åŠ¨ï¼
xtuner chat /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b \
  --visual-encoder /root/share/new_models/openai/clip-vit-large-patch14-336 \
  --llava /root/tutorial/xtuner/llava/llava_data/iter_1200_hf \
  --prompt-template internlm2_chat \
  --image /root/tutorial/xtuner/llava/llava_data/test_img/oph.jpg
```
> Q1: Describe this image.    
> Q2: What is the equipment in the image?

Finetuneå‰åæ•ˆæœå¯¹æ¯”ï¼š

**Finetuneå‰ï¼šåªä¼šæ‰“æ ‡é¢˜**
![](../images/lecture4/1717072547976.png)

**Finetuneåï¼šä¼šå›ç­”é—®é¢˜äº†**
![](../images/lecture4/1717080018073.png)


Bugè®°å½•ï¼šğŸ›ğŸ›ğŸ›
![](../images/lecture4/1613213465421.png)
â€œXTunerå¤šæ¨¡æ€è®­ç»ƒä¸æµ‹è¯•â€è®­ç»ƒæ¨¡å‹æŒ‡ä»¤æ‰§è¡ŒæŠ¥é”™TypeError: 'NoneType object is not subscriptable in xxxx

è§£å†³æ–¹æ¡ˆï¼š
transformersçš„åŒ…ä¸å¯¹ï¼ŒåŸæ¥çš„æ˜¯4.41.1ï¼Œæ”¹ä¸º4.40.0åº”è¯¥å°±OKäº†
```python
pip install transformers==4.40.0
```



